{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Artificial Neural Networks"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Artificial neural networks (ANNs) are computational models, inspired by the way the brain works, that are capable of machine learning and pattern recognition. The main advantage of ANNs is that they can learn complex non-linear hypotheses. They are usually presented as systems of interconnected \"neurons\" that can compute values from inputs by feeding information through the network.\n",
      "\n",
      "A single neuron can be modeled as a \"logistic unit,\" where a series of features and associated weights are passed through a sigmoid function. Neurons can also be modeled using other acitivation functions. The perceptron is a well-known type of artificial neuron that uses a threshold activation function. The output \"hypothesis\" of the artificial neuron is either 0 or 1, and for a logistic unit it is also known as the Sigmoid (logistic) activation function.\n",
      "\n",
      "A neural network is a group of neurons (here, logistic units) connected in layers. Traditionally, the first layer is called the \"input layer\" and the last layer the \"output layer,\" while any layers in between are called \"hidden layers.\" Each logistic unit in a layer has the same inputs, which are all the outputs from the previous layer, plus a bias input. The network has one neuron in the output layer for each possible classification class. The more hidden layers there are, the more complex (i.e. non-linear) relationships the network can learn.\n",
      "\n",
      "Each neuron uses a linear output that is the weighted sum of its input. Initially, before training, the weights are set the random values. Then, an initial prediction from the neural network is computed by \"forward propagation,\" where the output of neurons at earlier layers are used as the input to neurons at later ones. Since the input weights are initially random, the network will compute an output that very likely differs from the actual output. A common method for measuring the discrepancy between the expected output and the actual output is the squared error measure (the square of the difference). Given this measure of error, the problem of mapping inputs to outputs can be reduced to an optimization problem of finding a function (combination of weights) that will produce the minimal error. As the output of a neuron depends on the weighted sum of all its inputs, the error also depends on the incoming weights to the neuron. Thus, the minimization of the error is computed by the \"backpropagation\" algorithm, which propagates the error backwards through the network layers. As the error is propagated backwards, small changes are made to the weights in each layer that are calculated to reduce the error. The whole process proceeds iteratively over each instance in the dataset, and can be repeated until the overall error value drops below some pre-determined threshold. \n",
      "\n",
      "Here, we generate an ANN with one hidden layer."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (c) 2014 Reid Johnson\n",
      "#\n",
      "# Modified from:\n",
      "# Daniel Rodriguez (http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/)\n",
      "#\n",
      "# Functions to generate an artifical neural network (ANN) with one hidden \n",
      "# layer. The ANN is a collection of artificial neurons arranged in layers \n",
      "# that are trained using backpropogation.\n",
      "#\n",
      "# The perceptron algorithm is defined by Rosenblatt in:\n",
      "# The Perceptron: A Probalistic Model for Information Storage and Organization in the Brain.\n",
      "# Psychological Review 65 (6): 386\u2013408. 1958.\n",
      "#\n",
      "# The backpropogation algorithm is defined by Werbos in:\n",
      "# Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. 1975.\n",
      "\n",
      "import numpy as np\n",
      "from scipy import optimize\n",
      "from __future__ import division\n",
      "\n",
      "class NN_1HL(object):\n",
      "    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):\n",
      "        self.reg_lambda = reg_lambda # weight for the logistic regression cost\n",
      "        self.epsilon_init = epsilon_init # learning rate\n",
      "        self.hidden_layer_size = hidden_layer_size # size of the hidden layer\n",
      "        self.activation_func = self.sigmoid # activation function\n",
      "        self.activation_func_prime = self.sigmoid_prime # derivative of the activation function\n",
      "        self.method = opti_method # optimization method\n",
      "        self.maxiter = maxiter # maximum number of iterations\n",
      "\n",
      "    def sigmoid(self, z):\n",
      "        ''' Logistic function.\n",
      "\n",
      "        Returns: \n",
      "          The logistic function.\n",
      "\n",
      "        '''\n",
      "        return 1 / (1 + np.exp(-z))\n",
      "\n",
      "    def sigmoid_prime(self, z):\n",
      "        ''' Derivative of the logistic function.\n",
      "\n",
      "        Returns: \n",
      "          The derivative of the logistic function.\n",
      "\n",
      "        '''\n",
      "        sig = self.sigmoid(z)\n",
      "\n",
      "        return sig * (1-sig)\n",
      "\n",
      "    def sumsqr(self, a):\n",
      "        ''' Sum of squared values.\n",
      "\n",
      "        Returns: \n",
      "          The sum of squared values.\n",
      "\n",
      "        '''\n",
      "        return np.sum(a**2)\n",
      "\n",
      "    def rand_init(self, l_in, l_out):\n",
      "        ''' Generates an (l_out x l_in+1) array of thetas (threshold \n",
      "        values), each initialized to a random number between minus \n",
      "        epsilon and epsilon.\n",
      "\n",
      "        Note that there is one theta matrix per layer. The size of \n",
      "        each theta matrix depends on the number of activation units \n",
      "        in its corresponding layer, so each matrix may be of a \n",
      "        different size.\n",
      "\n",
      "        Returns:\n",
      "          Randomly initialized thetas (threshold values).\n",
      "\n",
      "        '''\n",
      "        return np.random.rand(l_out, l_in+1) * 2 * self.epsilon_init - self.epsilon_init\n",
      "\n",
      "    # Pack thetas (threshold values) into a one-dimensional array.\n",
      "    def pack_thetas(self, t1, t2):\n",
      "        ''' Packs (unrolls) thetas (threshold values) from matrices \n",
      "        into a single vector.\n",
      "\n",
      "        Note that there is one theta matrix per layer. To use an \n",
      "        optimization technique that minimizes the error, we need to \n",
      "        pack (unroll) the matrices into a single vector.\n",
      "\n",
      "        Args:\n",
      "          t1 (array): Unpacked (rolled) thetas (threshold values) between the input layer and hidden layer.\n",
      "          t2 (array): Unpacked (rolled) thetas (threshold values) between the hidden layer and output layer.\n",
      "\n",
      "        Returns:\n",
      "          Packed (unrolled) thetas (threshold values).\n",
      "\n",
      "        '''\n",
      "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
      "\n",
      "    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):\n",
      "        ''' Unpacks (rolls) thetas (threshold values) from a single\n",
      "        vector into a multi-dimensional array (matrices).\n",
      "\n",
      "         Args:\n",
      "          thetas (vector): Packed (unrolled) thetas (threshold values).\n",
      "          input_layer_size (int): Number of nodes in the input layer.\n",
      "          hidden_layer_size (int): Number of nodes in the hidden layer.\n",
      "          num_labels (int): Number of classes.\n",
      "\n",
      "        Returns:\n",
      "          t1 (array): Unpacked (rolled) thetas (threshold values) between the input layer and hidden layer.\n",
      "          t2 (array): Unpacked (rolled) thetas (threshold values) between the hidden layer and output layer.\n",
      "\n",
      "        '''\n",
      "        t1_start = 0\n",
      "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
      "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
      "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
      "\n",
      "        return t1, t2\n",
      "\n",
      "    def _forward(self, X, t1, t2):\n",
      "        ''' Forward propogation.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output probabilities.\n",
      "          t1 (array): Unpacked (rolled) thetas (threshold values) between the input layer and hidden layer.\n",
      "          t2 (array): Unpacked (rolled) thetas (threshold values) between the hidden layer and output layer.\n",
      "\n",
      "        Returns:\n",
      "          a1: The output activation of units in the input layer.\n",
      "          z2: The input activation of units in the hidden layer.\n",
      "          a2: The output activation of units in the hidden layer.\n",
      "          z3: The input activation of units in the output layer.\n",
      "          a3: The output activation of units in the output layer.\n",
      "\n",
      "        '''\n",
      "        m = X.shape[0]\n",
      "        ones = None\n",
      "        if len(X.shape) == 1:\n",
      "            ones = np.array(1).reshape(1,)\n",
      "        else:\n",
      "            ones = np.ones(m).reshape(m,1)\n",
      "\n",
      "        # Input layer.\n",
      "        a1 = np.hstack((ones, X))\n",
      "\n",
      "        # Hidden Layer.\n",
      "        z2 = np.dot(t1, a1.T)\n",
      "        a2 = self.activation_func(z2)\n",
      "        a2 = np.hstack((ones, a2.T))\n",
      "\n",
      "        # Output layer.\n",
      "        z3 = np.dot(t2, a2.T)\n",
      "        a3 = self.activation_func(z3)\n",
      "\n",
      "        return a1, z2, a2, z3, a3\n",
      "\n",
      "    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        ''' Generates the cost using a generalization of the regularized logistic regression cost function. \n",
      "\n",
      "        Returns:\n",
      "          J: the cost vector for each output unit.\n",
      "\n",
      "        '''\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "\n",
      "        m = X.shape[0]\n",
      "        Y = np.eye(num_labels)[y]\n",
      "\n",
      "        _, _, _, _, h = self._forward(X, t1, t2)\n",
      "        costPositive = -Y * np.log(h).T # the cost when y is 1\n",
      "        costNegative = (1 - Y) * np.log(1 - h).T # the cost when y is 0\n",
      "        cost = costPositive - costNegative # the total cost\n",
      "        J = np.sum(cost) / m # the (unregularized) cost function\n",
      "\n",
      "        # For regularization.\n",
      "        if reg_lambda != 0:\n",
      "            t1f = t1[:, 1:]\n",
      "            t2f = t2[:, 1:]\n",
      "            reg = (self.reg_lambda / (2*m)) * (self.sumsqr(t1f) + self.sumsqr(t2f)) # regularization term\n",
      "            J = J + reg # the regularized cost function\n",
      "\n",
      "        return J\n",
      "\n",
      "    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        ''' Generates the objective function to minimize based upon the cost function. \n",
      "\n",
      "        Returns:\n",
      "          Packed (unrolled) gradients for each theta.\n",
      "\n",
      "          '''\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "\n",
      "        m = X.shape[0]\n",
      "        t1f = t1[:, 1:] # threshold values between the input layer and hidden layer (excluding the bias input)\n",
      "        t2f = t2[:, 1:] # threshold values between the hidden layer and output layer (excluding the bias input)\n",
      "        Y = np.eye(num_labels)[y]\n",
      "\n",
      "        Delta1, Delta2 = 0, 0 # initialize matrix Deltas (cost function gradients)\n",
      "        # Iterate over the instances.\n",
      "        for i, row in enumerate(X):\n",
      "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
      "\n",
      "            # Backprop.\n",
      "            d3 = a3 - Y[i, :].T # activation error (delta) in the output layer nodes\n",
      "            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2) # activation error (delta) in the hidden layer nodes\n",
      "\n",
      "            # Update matrix Deltas (cost function gradients).\n",
      "            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
      "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
      "\n",
      "        # The (unregularized) gradients for each theta.\n",
      "        Theta1_grad = (1 / m) * Delta1 \n",
      "        Theta2_grad = (1 / m) * Delta2\n",
      "\n",
      "        # For regularization.\n",
      "        if reg_lambda != 0:\n",
      "            # The regularized gradients for each theta (excluding the bias input).\n",
      "            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
      "            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
      "\n",
      "        return self.pack_thetas(Theta1_grad, Theta2_grad)\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        ''' Fit the model given predictor(s) X and target y.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output.\n",
      "          y ((n,1) array): The actual outputs (class data).\n",
      "\n",
      "        '''\n",
      "        num_features = X.shape[0]\n",
      "        input_layer_size = X.shape[1]\n",
      "        num_labels = len(set(y))\n",
      "\n",
      "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
      "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
      "        thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
      "\n",
      "        options = {'maxiter': self.maxiter}\n",
      "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n",
      "                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)\n",
      "\n",
      "        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)\n",
      "\n",
      "    def predict(self, X):\n",
      "        ''' Predict labels with the fitted model on predictor(s) X.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted outputs.\n",
      "\n",
      "        Returns:\n",
      "          The predicted labels for each instance X.\n",
      "    \n",
      "        '''\n",
      "        return self.predict_proba(X).argmax(0)\n",
      "\n",
      "    def predict_proba(self, X):\n",
      "        ''' Predict label probabilities with the fitted model on predictor(s) X.\n",
      "\n",
      "        The probabilities are computed as the output activation of units in the output layer.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output probabilities.\n",
      "\n",
      "        Returns:\n",
      "          h: The predicted label probabilities for each instance X.\n",
      "\n",
      "        '''\n",
      "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
      "\n",
      "        return h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here, we will demonstrate ANNs by using the Iris flower dataset. Thus, we first load and perform some preprocessing on the data. The preprocessing involves altering the target or class variables, which in the Iris dataset are by default represented as strings (nominal values), but for compatibility reasons need to be represented as integers (numeric values). We perform this conversion using a label-encoding method available via scikit-learn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from sklearn import cross_validation\n",
      "from sklearn import preprocessing\n",
      "\n",
      "label_encode = True\n",
      "n_folds = 5\n",
      "\n",
      "# Load the Iris flower dataset\n",
      "fileURL = 'http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
      "iris = pd.read_csv(fileURL, \n",
      "                   names=['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width', 'Species'],\n",
      "                   header=None)\n",
      "iris = iris.dropna()\n",
      "\n",
      "X = np.array(iris[['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']]) # features\n",
      "y = iris['Species'] # class\n",
      "\n",
      "if label_encode:\n",
      "    # Transform string (nominal) output to numeric\n",
      "    labels = preprocessing.LabelEncoder().fit_transform(y)\n",
      "else:\n",
      "    labels = y\n",
      "\n",
      "# Generate k stratified folds of the data.\n",
      "skf = list(cross_validation.StratifiedKFold(labels, n_folds))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, we train the ANN classifier, as well as a Random Forest classifier for comparison. We use crossfold-validation to train the classifiers, which we evaluate using accuracy. For each classifier, the accuracy over all folds are averaged to generate the classifier's overall accuracy. We also use the timeit magic function build-in to IPython Notebook to track the training time for each classifier during each fold."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn import metrics\n",
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "# Generate classification objects.\n",
      "nn = NN_1HL()\n",
      "rfc = RandomForestClassifier(n_estimators=50)\n",
      "\n",
      "# Generate arrays for meta-level training and testing sets, which are n x len(clfs).\n",
      "scores_nn = np.zeros(n_folds) # scores for nn\n",
      "scores_rfc = np.zeros(n_folds) # scores for rfc\n",
      "\n",
      "print 'Training classifiers...'\n",
      "# Iterate over the folds, each with training set and validation set indicies.\n",
      "for i, (train_index, test_index) in enumerate(skf):\n",
      "    print '  Fold [%s]' % (i)\n",
      "\n",
      "    # Generate the training set for the fold.\n",
      "    X_train = X[train_index]\n",
      "    y_train = labels[train_index]\n",
      "\n",
      "    # Generate the testing set for the fold.\n",
      "    X_test = X[test_index]\n",
      "    y_test = labels[test_index]\n",
      "\n",
      "    # Train the models on the training set.\n",
      "    # We time the training using the built-in timeit magic function.\n",
      "    print '    Neural Network: ',\n",
      "    %timeit nn.fit(X_train, y_train)\n",
      "    print '    Random Forest: ',\n",
      "    %timeit rfc.fit(X_train, y_train)\n",
      "\n",
      "    # Evaluate the models on the testing set.\n",
      "    scores_nn[i] = metrics.accuracy_score(y_test, nn.predict(X_test))\n",
      "    scores_rfc[i] = metrics.accuracy_score(y_test, rfc.predict(X_test))\n",
      "print 'Done training classifiers.'\n",
      "print\n",
      "\n",
      "# The mean of the scores on the testing set.\n",
      "print 'Artificial Neural Network Accuracy = %s' % (scores_nn.mean(axis=0))\n",
      "print 'Random Forest Accuracy = %s' % (scores_rfc.mean(axis=0))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training classifiers...\n",
        "  Fold [0]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 2.12 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 46.6 ms per loop\n",
        "  Fold [1]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 2.3 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 34.1 ms per loop\n",
        "  Fold [2]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 1.82 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 52.8 ms per loop\n",
        "  Fold [3]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 2.63 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 52 ms per loop\n",
        "  Fold [4]\n",
        "    Neural Network:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1 loops, best of 3: 2.3 s per loop\n",
        "    Random Forest:  "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "10 loops, best of 3: 40.1 ms per loop\n",
        "Done training classifiers.\n",
        "\n",
        "Artificial Neural Network Accuracy = 0.946666666667\n",
        "Random Forest Accuracy = 0.933333333333\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that even for this small dataset, an artificial neural network with one hidden layer takes a comparatively long time to train. While the implementation is undoubtly less optimized than algorithms available via scikit-learn, the current artificial neural network models are known to be comparatively slow. At the same time, the are also known for generating comparatively accurate predictions."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
