{
 "metadata": {
  "name": "",
  "signature": "sha256:6cb0f56ae72fb617b08a5ba46367732fa866246d9cb81b7322b5b50c886d410e"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Ejercicio 4 \n",
      "\n",
      "La idea de este ejercicio es estudiar el fen\u00f3meno de overfitting. Supongamos que tenemos una serie\n",
      "de datos que vamos ajustar con una red (una capa hidden). Los datos se encuentran en el archivo\n",
      "*datos_guia4_ej4.mat*. Nuestro objetivo es determinar el n\u00famero optimo de neuronas en la capa\n",
      "hidden.\n",
      "\n",
      "* a) Determine una estrategia para dividir los datos en dos conjuntos: {ptrain,ttrain} y {ptest,ttest}\n",
      "\n",
      "\n",
      "* b) Ajuste Nest redes neuronales con Nneuro neuronas en la capa hidden\n",
      "\n",
      "* c) Grafique el error absoluto promedio para los ajustes de entrenamiento {ptrain,ttrain} y para los ajustes con los datos de testeo {ptest,ttest}.\n",
      "\n",
      "* d) Explique el comportamiento de ambos errores.\n",
      "\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import scipy.io as sio\n",
      "%matplotlib inline\n",
      "plt.rcParams['figure.figsize'] = 8,6 #par\u00e1metros de tama\u00f1o"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 186
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Abrimos los archivos de datos\n",
      "datos = sio.loadmat('datos_guia4_ej4.mat')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 187
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t = datos['t']\n",
      "p = datos['p']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 188
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#size_t = t.shape\n",
      "#size_p = p.shape\n",
      "#t = t.reshape(size_t[1])\n",
      "#p = p.reshape(size_p[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 189
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#plt.plot(p)\n",
      "#plt.plot(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 190
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# (c) 2014 Reid Johnson\n",
      "#\n",
      "# Modified from:\n",
      "# Daniel Rodriguez (http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/)\n",
      "#\n",
      "# Functions to generate an artifical neural network (ANN) with one hidden \n",
      "# layer. The ANN is a collection of artificial neurons arranged in layers \n",
      "# that are trained using backpropogation.\n",
      "#\n",
      "# The perceptron algorithm is defined by Rosenblatt in:\n",
      "# The Perceptron: A Probalistic Model for Information Storage and Organization in the Brain.\n",
      "# Psychological Review 65 (6): 386\u2013408. 1958.\n",
      "#\n",
      "# The backpropogation algorithm is defined by Werbos in:\n",
      "# Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. 1975.\n",
      "\n",
      "\n",
      "from scipy import optimize\n",
      "from __future__ import division\n",
      "\n",
      "class NN_1HL(object):\n",
      "    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):\n",
      "        self.reg_lambda = reg_lambda # weight for the logistic regression cost\n",
      "        self.epsilon_init = epsilon_init # learning rate\n",
      "        self.hidden_layer_size = hidden_layer_size # size of the hidden layer\n",
      "        self.activation_func = self.sigmoid # activation function\n",
      "        self.activation_func_prime = self.sigmoid_prime # derivative of the activation function\n",
      "        self.method = opti_method # optimization method\n",
      "        self.maxiter = maxiter # maximum number of iterations\n",
      "\n",
      "    def sigmoid(self, z):\n",
      "        ''' Logistic function.\n",
      "\n",
      "        Returns: \n",
      "          The logistic function.\n",
      "\n",
      "        '''\n",
      "        return 1 / (1 + np.exp(-z))\n",
      "\n",
      "    def sigmoid_prime(self, z):\n",
      "        ''' Derivative of the logistic function.\n",
      "\n",
      "        Returns: \n",
      "          The derivative of the logistic function.\n",
      "\n",
      "        '''\n",
      "        sig = self.sigmoid(z)\n",
      "\n",
      "        return sig * (1-sig)\n",
      "\n",
      "    def sumsqr(self, a):\n",
      "        ''' Sum of squared values.\n",
      "\n",
      "        Returns: \n",
      "          The sum of squared values.\n",
      "\n",
      "        '''\n",
      "        return np.sum(a**2)\n",
      "\n",
      "    def rand_init(self, l_in, l_out):\n",
      "        ''' Generates an (l_out x l_in+1) array of thetas (threshold \n",
      "        values), each initialized to a random number between minus \n",
      "        epsilon and epsilon.\n",
      "\n",
      "        Note that there is one theta matrix per layer. The size of \n",
      "        each theta matrix depends on the number of activation units \n",
      "        in its corresponding layer, so each matrix may be of a \n",
      "        different size.\n",
      "\n",
      "        Returns:\n",
      "          Randomly initialized thetas (threshold values).\n",
      "\n",
      "        '''\n",
      "        return np.random.rand(l_out, l_in+1) * 2 * self.epsilon_init - self.epsilon_init\n",
      "\n",
      "    # Pack thetas (threshold values) into a one-dimensional array.\n",
      "    def pack_thetas(self, t1, t2):\n",
      "        ''' Packs (unrolls) thetas (threshold values) from matrices \n",
      "        into a single vector.\n",
      "\n",
      "        Note that there is one theta matrix per layer. To use an \n",
      "        optimization technique that minimizes the error, we need to \n",
      "        pack (unroll) the matrices into a single vector.\n",
      "\n",
      "        Args:\n",
      "          t1 (array): Unpacked (rolled) thetas (threshold values) between the input layer and hidden layer.\n",
      "          t2 (array): Unpacked (rolled) thetas (threshold values) between the hidden layer and output layer.\n",
      "\n",
      "        Returns:\n",
      "          Packed (unrolled) thetas (threshold values).\n",
      "\n",
      "        '''\n",
      "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
      "\n",
      "    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):\n",
      "        ''' Unpacks (rolls) thetas (threshold values) from a single\n",
      "        vector into a multi-dimensional array (matrices).\n",
      "\n",
      "         Args:\n",
      "          thetas (vector): Packed (unrolled) thetas (threshold values).\n",
      "          input_layer_size (int): Number of nodes in the input layer.\n",
      "          hidden_layer_size (int): Number of nodes in the hidden layer.\n",
      "          num_labels (int): Number of classes.\n",
      "\n",
      "        Returns:\n",
      "          t1 (array): Unpacked (rolled) thetas (threshold values) between the input layer and hidden layer.\n",
      "          t2 (array): Unpacked (rolled) thetas (threshold values) between the hidden layer and output layer.\n",
      "\n",
      "        '''\n",
      "        t1_start = 0\n",
      "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
      "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
      "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
      "\n",
      "        return t1, t2\n",
      "\n",
      "    def _forward(self, X, t1, t2):\n",
      "        ''' Forward propogation.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output probabilities.\n",
      "          t1 (array): Unpacked (rolled) thetas (threshold values) between the input layer and hidden layer.\n",
      "          t2 (array): Unpacked (rolled) thetas (threshold values) between the hidden layer and output layer.\n",
      "\n",
      "        Returns:\n",
      "          a1: The output activation of units in the input layer.\n",
      "          z2: The input activation of units in the hidden layer.\n",
      "          a2: The output activation of units in the hidden layer.\n",
      "          z3: The input activation of units in the output layer.\n",
      "          a3: The output activation of units in the output layer.\n",
      "\n",
      "        '''\n",
      "        m = X.shape[0]\n",
      "        ones = None\n",
      "        if len(X.shape) == 1:\n",
      "            ones = np.array(1).reshape(1,)\n",
      "        else:\n",
      "            ones = np.ones(m).reshape(m,1)\n",
      "\n",
      "        # Input layer.\n",
      "        a1 = np.hstack((ones, X))\n",
      "\n",
      "        # Hidden Layer.\n",
      "        z2 = np.dot(t1, a1.T)\n",
      "        a2 = self.activation_func(z2)\n",
      "        a2 = np.hstack((ones, a2.T))\n",
      "\n",
      "        # Output layer.\n",
      "        z3 = np.dot(t2, a2.T)\n",
      "        a3 = self.activation_func(z3)\n",
      "\n",
      "        return a1, z2, a2, z3, a3\n",
      "\n",
      "    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        ''' Generates the cost using a generalization of the regularized logistic regression cost function. \n",
      "\n",
      "        Returns:\n",
      "          J: the cost vector for each output unit.\n",
      "\n",
      "        '''\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "\n",
      "        m = X.shape[0]\n",
      "        Y = np.eye(num_labels)[y]\n",
      "\n",
      "        _, _, _, _, h = self._forward(X, t1, t2)\n",
      "        costPositive = -Y * np.log(h).T # the cost when y is 1\n",
      "        costNegative = (1 - Y) * np.log(1 - h).T # the cost when y is 0\n",
      "        cost = costPositive - costNegative # the total cost\n",
      "        J = np.sum(cost) / m # the (unregularized) cost function\n",
      "\n",
      "        # For regularization.\n",
      "        if reg_lambda != 0:\n",
      "            t1f = t1[:, 1:]\n",
      "            t2f = t2[:, 1:]\n",
      "            reg = (self.reg_lambda / (2*m)) * (self.sumsqr(t1f) + self.sumsqr(t2f)) # regularization term\n",
      "            J = J + reg # the regularized cost function\n",
      "\n",
      "        return J\n",
      "\n",
      "    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        ''' Generates the objective function to minimize based upon the cost function. \n",
      "\n",
      "        Returns:\n",
      "          Packed (unrolled) gradients for each theta.\n",
      "\n",
      "          '''\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "\n",
      "        m = X.shape[0]\n",
      "        t1f = t1[:, 1:] # threshold values between the input layer and hidden layer (excluding the bias input)\n",
      "        t2f = t2[:, 1:] # threshold values between the hidden layer and output layer (excluding the bias input)\n",
      "        Y = np.eye(num_labels)[y]\n",
      "\n",
      "        Delta1, Delta2 = 0, 0 # initialize matrix Deltas (cost function gradients)\n",
      "        # Iterate over the instances.\n",
      "        for i, row in enumerate(X):\n",
      "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
      "\n",
      "            # Backprop.\n",
      "            d3 = a3 - Y[i, :].T # activation error (delta) in the output layer nodes\n",
      "            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2) # activation error (delta) in the hidden layer nodes\n",
      "\n",
      "            # Update matrix Deltas (cost function gradients).\n",
      "            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
      "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
      "\n",
      "        # The (unregularized) gradients for each theta.\n",
      "        Theta1_grad = (1 / m) * Delta1 \n",
      "        Theta2_grad = (1 / m) * Delta2\n",
      "\n",
      "        # For regularization.\n",
      "        if reg_lambda != 0:\n",
      "            # The regularized gradients for each theta (excluding the bias input).\n",
      "            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
      "            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
      "\n",
      "        return self.pack_thetas(Theta1_grad, Theta2_grad)\n",
      "\n",
      "    def fit(self, X, y):\n",
      "        ''' Fit the model given predictor(s) X and target y.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output.\n",
      "          y ((n,1) array): The actual outputs (class data).\n",
      "\n",
      "        '''\n",
      "        num_features = X.shape[1]\n",
      "        input_layer_size = X.shape[0]\n",
      "        num_labels = int(len(y))\n",
      "\n",
      "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
      "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
      "        thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
      "\n",
      "        options = {'maxiter': self.maxiter}\n",
      "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n",
      "                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)\n",
      "\n",
      "        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)\n",
      "\n",
      "    def predict(self, X):\n",
      "        ''' Predict labels with the fitted model on predictor(s) X.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted outputs.\n",
      "\n",
      "        Returns:\n",
      "          The predicted labels for each instance X.\n",
      "    \n",
      "        '''\n",
      "        return self.predict_proba(X).argmax(0)\n",
      "\n",
      "    def predict_proba(self, X):\n",
      "        ''' Predict label probabilities with the fitted model on predictor(s) X.\n",
      "\n",
      "        The probabilities are computed as the output activation of units in the output layer.\n",
      "\n",
      "        Args:\n",
      "          X ((n,m) array): The feature data for which to compute the predicted output probabilities.\n",
      "\n",
      "        Returns:\n",
      "          h: The predicted label probabilities for each instance X.\n",
      "\n",
      "        '''\n",
      "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
      "\n",
      "        return h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 191
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn = NN_1HL(hidden_layer_size=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 192
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nn.fit(p.T,t.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "arrays used as indices must be of integer (or boolean) type",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-193-d3c481ff68bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-191-7c025a046de9>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[0moptions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'maxiter'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n\u001b[1;32m--> 238\u001b[1;33m                                  args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munpack_thetas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_layer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_layer_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/_minimize.pyc\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tnc'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n\u001b[1;32m--> 430\u001b[1;33m                              **options)\n\u001b[0m\u001b[0;32m    431\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'cobyla'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_cobyla\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconstraints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/tnc.pyc\u001b[0m in \u001b[0;36m_minimize_tnc\u001b[1;34m(fun, x0, args, jac, bounds, eps, scale, offset, mesg_num, maxCGit, maxiter, eta, stepmx, accuracy, minfev, ftol, xtol, gtol, rescale, disp, callback, **unknown_options)\u001b[0m\n\u001b[0;32m    396\u001b[0m                                         \u001b[0moffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxCGit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxfun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m                                         \u001b[0meta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstepmx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mftol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 398\u001b[1;33m                                         xtol, pgtol, rescale, callback)\n\u001b[0m\u001b[0;32m    399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[0mfunv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjacv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/tnc.pyc\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 360\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    361\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m<ipython-input-191-7c025a046de9>\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 164\u001b[1;33m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
       ]
      }
     ],
     "prompt_number": 193
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p.T.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t.T.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(t.T)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "t.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(t)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = p.T.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}